{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5149 S2 2019 Assessment : Sentiment Classification for Product Reviews\n",
    "\n",
    "Group information\n",
    "- Group No: 52\n",
    "\n",
    "Programming Language: Python in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this data analysis challenge, we are interested in developing such an automatic sentiment classification system that relies on machine learning techniques to learn from a large set of product reviews provided by Yelp.\n",
    "#### The levels of polarity of opinion we consider include strong negative, weak negative, neutral, weak positive, and strong positive. \n",
    "#### The aim of this challenge is to develop a sentiment classifier that can assign a large set of product reviews to the five levels of polarity of opinion as accurately as possible, given a small amount of labeled reviews and a large amount of unlabelled reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Steps performed for building Sentiment classifier: <br>\n",
    "1.Pre-processing and cleaning the labeled, unlabeled and test data.<br>\n",
    "2.Creating features using Count Vectorizer. <br>\n",
    "3.Applying Logistic Regression to the labeled data. <br>\n",
    "4.Retraining the model on labeled and certain % of unlabeled data. <br>\n",
    "5.Predicting labels for test data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages.\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# import packages for pre-processing \n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk.data\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import packages for pre-processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# import packages for pipielining\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-41355ce896b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabel_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labeled_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munlabeled_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unlabeled_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "label_data = pd.read_csv(\"labeled_data.csv\",encoding='utf-8')        # reading label data\n",
    "unlabeled_data = pd.read_csv(\"unlabeled_data.csv\",encoding='utf-8')  # reading unlabeled data\n",
    "test_data = pd.read_csv(\"test_data.csv\",encoding='utf-8')            # reading test_data data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing and cleaning data is a vital part of data analysis, text data contains a lot of unwanted words which are not important or are special or new characters. <br> \n",
    "Below given function,\n",
    "1. Removes uncodeable characters, retaining rest of the data.\n",
    "2. Removes new line characters.\n",
    "3. Removes unwanted space, special characters.\n",
    "4. Stemming of the data, basically trims the words, hence extracting the orginal words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning text.\n",
    "def clean_text1(data):\n",
    "    \n",
    "    clean=data['text'].encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    clean=clean.lower()\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\\\[rn]', ' ',clean)\n",
    "    processed_feature = re.sub(r'[^a-zA-Z0-9]+',' ',processed_feature)\n",
    "    word_list = processed_feature.split(\" \")\n",
    "    \n",
    "    # steming the data\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_list = []\n",
    "    for word in word_list:\n",
    "        stemmed_list.append(stemmer.stem(word))\n",
    "    \n",
    "    text = \" \".join(stemmed_list)\n",
    "   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the data\n",
    "label_data['clean_text'] = label_data.apply(clean_text1,axis=1)\n",
    "label_data['ID'] = \"Labeled\"\n",
    "label_data['test_id'] = \"\"      # new-column for test id.\n",
    "\n",
    "# pre-processing the data\n",
    "unlabeled_data['clean_text'] = unlabeled_data.apply(clean_text1,axis=1)\n",
    "unlabeled_data['ID'] = \"Unlabeled\"\n",
    "unlabeled_data['label'] = \"\"\n",
    "unlabeled_data['test_id'] = \"\"  # new-column for test id\n",
    "\n",
    "# pre-processing the data\n",
    "test_data['clean_text'] = test_data.apply(clean_text1,axis=1)\n",
    "test_data['ID'] = \"Test\"\n",
    "test_data['label'] = \"\"         # new-column for test id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Model building, we need to first prepare our data according to the input criteria of the algorithms.\n",
    "Following are steps performed for preparing the data and model building: -\n",
    "1. Clean text converted into features using Count vectorizer.\n",
    "2. Removing stop words, using tokens of of 3 or more characters, forming uni-bi-trigrams.\n",
    "3. Applying Logistic regression to the labeled data, then implementing it to the unlabeled data to predict the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Semi-Supervised learning, we often encounter small label data and large unlabeled data. <br> ** Hence, after predicting the labels for unlabeled data, we will be including 1 lakh rows of unlabeled data with labels into the labeled data and again train it. <br>\n",
    "Label data contained 50k rows, therefore better training of the model, we took 1 lakh rows.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrained model is then implemented on the given test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'clean_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b07acee1c6e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_unlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munlabeled_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'clean_text'"
     ]
    }
   ],
   "source": [
    "# creating train and test for model building.\n",
    "X_train = label_data.clean_text\n",
    "y_train = label_data.label\n",
    "\n",
    "X_unlabel = unlabeled_data.clean_text\n",
    "X_test = test_data.clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We would be implementing feature generation i.e Count Vectorizer and Logistic Regression in a pipieline to process the code faster.\n",
    "- Steps 1 and 2 are performed with the parameters provided in the Count Vectorizer.\n",
    "- n_jobs distributes the process into 4 cores, C value penalises the data for larger no of features, 'lbfgs' method is better for multiclass classification as well as larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model through a pipeline.\n",
    "logreg = Pipeline([\n",
    "                \n",
    "                # converts clean text into bag of words.\n",
    "                ('cntvector', CountVectorizer(stop_words='english', # removes stop words from the text.\n",
    "                                    token_pattern=r'\\w{3,}',        # accept tokens that have 3 or more characters\n",
    "                                    analyzer='word',\n",
    "                                    ngram_range=(1, 3))),           # forms uni, bi, and trigrams\n",
    "    \n",
    "                # appplying logistic regression\n",
    "                ('clf', LogisticRegression(n_jobs=4, C=0.5, solver='lbfgs',multi_class='multinomial')),\n",
    "               ])\n",
    "\n",
    "# Fit the model to the data.\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# predicting on the unlabeled data.\n",
    "y_unlabel = logreg.predict(X_unlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the predicted data to the label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_data = unlabeled_data\n",
    "new_label_data['label'] = y_unlabel          # appending the predicted values.\n",
    "\n",
    "combine = label_data\n",
    "combine=combine.append(new_label_data[:100000], ignore_index = True)   # concat new_label_data with label_data.\n",
    "\n",
    "# preparing x,y for retraining the model.\n",
    "X_combine = combine.clean_text\n",
    "y_combine = combine.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the new data.\n",
    "logreg.fit(X_combine, y_combine)\n",
    "\n",
    "# predicting on the given test data.\n",
    "y_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparaing data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = test_data\n",
    "eval_data['label'] = y_test     # appending the prediction column to the test data\n",
    "\n",
    "final = eval_data[['test_id','label']]    # renaming the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"predict_label.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
